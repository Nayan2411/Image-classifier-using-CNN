{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcCbBKCAc1yG"
   },
   "source": [
    "# Deep learning: An introduction through image processing\n",
    "## Pictures classification\n",
    "\n",
    "create a neural network capable of classifying dog and cat pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnEeOvaLO4gx"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54ezBYCaAdhI",
    "outputId": "e4b18e00-3107-4b02-cf2d-0858e93e009f"
   },
   "outputs": [],
   "source": [
    "# Download a zip file with the cats and dogs images\n",
    "# The structure of the folders in the zip file is:\n",
    "# - train\n",
    "#   - cats\n",
    "#     - cat.O.jpg\n",
    "#     - cat.1.jpg\n",
    "#     - ...\n",
    "#   - dogs\n",
    "#     - dog.O.jpg\n",
    "#     - dog.1.jpg\n",
    "#     - ...\n",
    "# - validation\n",
    "#   - cats\n",
    "#     - cat.200O.jpg\n",
    "#     - cat.2001.jpg\n",
    "#     - ...\n",
    "#   - dogs\n",
    "#     - dog.200O.jpg\n",
    "#     - dog.2001.jpg\n",
    "#     - ...\n",
    "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /tmp/cats_and_dogs_filtered.zip\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Unzip the zip file to /tmp location\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "# set the train dir to \n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9l2FVpddSqW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1WiLNvbBRi_"
   },
   "source": [
    "Now let's visualize some of our data thanks to the Matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "PgE5pZrVBQqf",
    "outputId": "49384c3a-0dba-468a-84cd-8501937cc64d"
   },
   "outputs": [],
   "source": [
    "# We take one of the pictures in the trainin set, among the dog pictures\n",
    "filename = train_cats_dir + '/' + 'cat.10.jpg'\n",
    "# Load the image in memory\n",
    "image = matplotlib.image.imread(filename)\n",
    "# print the image shape\n",
    "print(image.shape)\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duk2al-Nc8C5"
   },
   "source": [
    "We define some helper objects for the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JodujjmVcWLy",
    "outputId": "140943b8-4fe9-4153-bbbc-1528741d86a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The rescale of the values of the input image from [0, 255] to [0, 1] is done here.\n",
    "data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# We create that will automatically load the images from the training and validation folders, and zill pass them to the model during the training\n",
    "train_iterator = data_generator.flow_from_directory(train_dir, class_mode='categorical', batch_size=32, target_size=(128, 128))\n",
    "test_iterator = data_generator.flow_from_directory(validation_dir, class_mode='categorical', batch_size=32, target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP4hZW37eDsn"
   },
   "source": [
    "#Step 1: Define the simplest model\n",
    "\n",
    "As a first step, we're going to try to use the simplest neural network possible solve this problem. Let's see what accuracy we can reach for this task of classifying cats and dogs pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "E1WtVPb5eOLY",
    "outputId": "7a18e1ce-a5fc-4480-a7e2-ee52fb193a35"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(128, 128, 3)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgS83bzdP-lF"
   },
   "source": [
    "#Step 2: Define a more complex densely-connected model\n",
    "\n",
    "As a second step, try more complex neural network. What is the best accuracy that you can achieve?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jslz5TYLP9ry",
    "outputId": "f28b4da4-08b7-4a64-84d8-ee247c6ae3fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(128, 128, 3)))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paoqOuesj3IN"
   },
   "source": [
    "#First convolutional network\n",
    "\n",
    "The structure of the network we're going to create is:\n",
    "\n",
    "(128,128,3) --- convolution ---> (128,128,32) --- pooling ---> (64,64,32) --- dense ---> 32 ---> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jsxm7Ostnt1D",
    "outputId": "23e177ed-61de-4873-d86d-b497eb8aa2b4"
   },
   "outputs": [],
   "source": [
    "# The model is a sequential model, as usual so far\n",
    "model = Sequential()\n",
    "# First layer of neurons after the input layer in a convolutional layer, with 32 filters, of size 3x3x3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)))\n",
    "# Then we apply a pooling to reduce the size of the layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgYfK2yFRALb"
   },
   "source": [
    "Step 3: Visualize the training\n",
    "\n",
    "Define a **visualize_training** function to be able to plot the evolution of the losses and the accuracies during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ukmu1tZqRQM7",
    "outputId": "add2a89a-32da-4945-9108-9b3f09bd6b44"
   },
   "outputs": [],
   "source": [
    "def visualize_training(history):\n",
    "\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['training', 'validation'], loc='lower right')\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['training', 'validation'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "visualize_training(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DgEsvJU0OEW"
   },
   "source": [
    "##Visualize the output of each layer\n",
    "\n",
    "One good thing with convolutional layers is that it keeps the image structure and you can visualize the output of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHWVdQrq0t5g",
    "outputId": "155dc495-5d10-494a-f8fa-a1ae8d370e2c"
   },
   "outputs": [],
   "source": [
    "#Display the layers of our model\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fSVg27HFyuts",
    "outputId": "730230ed-6522-4f7a-a1b8-dca3159712bc"
   },
   "outputs": [],
   "source": [
    "# Create a submodel from the model we've just train, only with the first layer (the convolutional one)\n",
    "submodel = Model(model.layers[0].input, model.layers[0].output)\n",
    "# We need to compile it to be able to use it\n",
    "submodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Let's display a quick summary of the model\n",
    "submodel.summary()\n",
    "\n",
    "# Open a picture from the validation dataset\n",
    "filename = validation_dogs_dir + '/dog.2001.jpg'\n",
    "# Load the image from the file in memory\n",
    "image = cv2.imread(filename)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,(128,128))\n",
    "image = image / 255\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "# Feed the submodel into the submodel\n",
    "filtered_images = submodel.predict(image.reshape((1,128,128,3)))\n",
    "# Display the shape of the array output by the submodel\n",
    "print(filtered_images.shape)\n",
    "# Then finally plot the output of the submodel\n",
    "fig, axs = plt.subplots(8, 4, figsize=(30,30))\n",
    "for i in range(0,8):\n",
    "  for j in range(0,4):\n",
    "    k = 4*i+j\n",
    "    axs[i, j].imshow(filtered_images[0,:,:,k], cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW_pZBXCVBtU"
   },
   "source": [
    "#Step 4: Data augmentation\n",
    "\n",
    "Use Keras available data augmentation capabilities to improve your network accuracy. See [Keras documentation](https://keras.io/api/preprocessing/image/) to see how to use the ImageDataGenerator from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a_eP45HWVNBP",
    "outputId": "439b7050-9cdf-4cf4-e485-16121f928169"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_generator = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                    rotation_range=30,\n",
    "                                    zoom_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    width_shift_range= 0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    horizontal_flip= True)\n",
    "# We create that will automatically load the images from the training and validation folders, and zill pass them to the model during the training\n",
    "train_iterator = data_generator.flow_from_directory(train_dir, class_mode='categorical', batch_size=128, target_size=(128, 128))\n",
    "test_iterator = data_generator.flow_from_directory(validation_dir, class_mode='categorical', batch_size=128, target_size=(128, 128))\n",
    "### END CODE HERE\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_iterator, batch_size=32, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "lVXpjTtIYxDI",
    "outputId": "c97e49ec-fa0d-4e0e-a5c3-a8018be771cb"
   },
   "outputs": [],
   "source": [
    "visualize_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpDWWAHBUvNN"
   },
   "source": [
    "#Step 5: Experience deeper neural network designs\n",
    "\n",
    "Try to stack convolutional layers, and add layers in the densely connected part of the network.\n",
    "\n",
    "Plot the output of your different filters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc8PhW_8VjZn",
    "outputId": "c358862f-caaf-45a1-83ef-03a769944bc2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                    rotation_range=30,\n",
    "                                    zoom_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    width_shift_range= 0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    horizontal_flip= True)\n",
    "# We create that will automatically load the images from the training and validation folders, and zill pass them to the model during the training\n",
    "train_iterator = data_generator.flow_from_directory(train_dir, class_mode='categorical', batch_size=64, target_size=(128, 128))\n",
    "test_iterator = data_generator.flow_from_directory(validation_dir, class_mode='categorical', batch_size=64, target_size=(128, 128))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)\n",
    "\n",
    "complex_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inHC8rftNk6q",
    "outputId": "972920e8-982c-463c-cada-4657ce08ae91"
   },
   "outputs": [],
   "source": [
    "augmented_data_generator = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                    rotation_range=30,\n",
    "                                    zoom_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    width_shift_range= 0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    horizontal_flip= True)\n",
    "data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# We create that will automatically load the images from the training and validation folders, and zill pass them to the model during the training\n",
    "train_iterator = augmented_data_generator.flow_from_directory(train_dir, class_mode='categorical', batch_size=32, target_size=(128, 128))\n",
    "test_iterator = data_generator.flow_from_directory(validation_dir, class_mode='categorical', batch_size=32, target_size=(128, 128))\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj_CLzAI56ow"
   },
   "source": [
    "#Learning transfer\n",
    "\n",
    "Use the well-known VGG network to try to improve the qccurqcy of your network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFwVPoMk552s",
    "outputId": "98bb1f47-6c04-4e37-d203-4b55e4c7043b"
   },
   "outputs": [],
   "source": [
    "# Import the VGG model which is already - well - trained\n",
    "input_tensor = Input(shape=(128, 128, 3))\n",
    "vgg_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Display summary of the VGG model\n",
    "vgg_model.summary()\n",
    "\n",
    "# Flag all VGG layers as not trainable so that when we'll train the complete it doesn't try to adjust the parameters of those first layers\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftJL_BpA7OBr",
    "outputId": "ebf8d2e5-5659-44da-c1df-a0afc5e87635"
   },
   "outputs": [],
   "source": [
    "# Get the last layer of the VGG model\n",
    "vgg_output = vgg_model.layers[-1].output\n",
    "# Flatten this last layer\n",
    "output = tf.keras.layers.Flatten()(vgg_output)\n",
    "# Then construct the remaining of the model\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(2, activation='softmax')(output)\n",
    "# Create a model from the input to the output\n",
    "model = Model(input_tensor, output)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_iterator, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tCXANVtG7BS"
   },
   "source": [
    "#Step 6: Visualize the output of the different VGG layers\n",
    "\n",
    "Use the same processes we used before to display the output of the filters in the VGG layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xptksuj9HNd4",
    "outputId": "54bb446d-4339-4d95-fc91-042ad9c3f157"
   },
   "outputs": [],
   "source": [
    "#Checking in 4th layer\n",
    "submodel = Model(model.layers[0].input, model.layers[4].output)\n",
    "# We need to compile it to be able to use it\n",
    "submodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Let's display a quick summary of the model\n",
    "submodel.summary()\n",
    "\n",
    "# Open a picture from the validation dataset\n",
    "filename = validation_dogs_dir + '/dog.2001.jpg'\n",
    "# Load the image from the file in memory\n",
    "image = cv2.imread(filename)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,(128,128))\n",
    "image = image / 255\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "# Feed the submodel into the submodel\n",
    "filtered_images = submodel.predict(image.reshape((1,128,128,3)))\n",
    "# Display the shape of the array output by the submodel\n",
    "print(filtered_images.shape)\n",
    "# Then finally plot the output of the submodel\n",
    "fig, axs = plt.subplots(8, 4, figsize=(30,30))\n",
    "for i in range(0,8):\n",
    "  for j in range(0,4):\n",
    "    k = 4*i+j\n",
    "    axs[i, j].imshow(filtered_images[0,:,:,k], cmap='gray', vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7ti79h1zC25"
   },
   "source": [
    "#Step 7: Try to trick your neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z4LI9T3NISg"
   },
   "source": [
    "We create and train a model on a smaller input, only 64 by 64, for memory consumption reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEIARk6_NTqk",
    "outputId": "98721d2d-ab5d-4456-8a31-903eb40746cb"
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, rotation_range=20, zoom_range=0.2, shear_range=0.2, width_shift_range=0.2, height_shift_range=0.2)\n",
    "train_iterator = data_generator.flow_from_directory(train_dir, class_mode='categorical', batch_size=32, target_size=(64, 64))\n",
    "test_iterator = data_generator.flow_from_directory(validation_dir, class_mode='categorical', batch_size=32, target_size=(64, 64))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_iterator, batch_size=32, steps_per_epoch=len(train_iterator), validation_data=test_iterator, validation_steps=len(test_iterator), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSLa09OHNki3"
   },
   "source": [
    "We're going to see if, by slightly modifying the input images, we can confuse the neural network and make it classify a dog for a cat.\n",
    "\n",
    "First let's find an image correctly predicted by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "pWGhhQ8xzBIU",
    "outputId": "3d60d9cf-4cf0-4956-b768-d6f4d8a2ef55"
   },
   "outputs": [],
   "source": [
    "print(train_iterator.class_indices)\n",
    "\n",
    "filename = train_dogs_dir + '/' + train_dog_fnames[2]\n",
    "image = cv2.imread(filename)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,(64,64))\n",
    "image = image / 255\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "prediction = model.predict(image.reshape((1,64,64,3)))\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOeTFEppb04W"
   },
   "source": [
    "Then for each pixel, we compute if increasing the value of the pixel makes the prediction better or worse. If it makes it better, we then record a positive value for the derivative. If it makes it worse, we record a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7WKzGmm0yCl",
    "outputId": "61807826-dc71-42bc-9444-e4d2353c8639"
   },
   "outputs": [],
   "source": [
    "# Initial prediction of the model\n",
    "initial_prediction = model.predict(image.reshape((1,64,64,3)))[0][1]\n",
    "# By how much we will increase the pixel to detect a difference in the prediction of the model\n",
    "delta = 0.001\n",
    "# The derivate for each pixel, same shape os the original image\n",
    "derivative = np.zeros((64,64,3))\n",
    "\n",
    "# We loop on each color\n",
    "for k in range(0,3):  \n",
    "  print(f\"Generating differential images for color {k}...\")\n",
    "  image_modified = np.zeros((64 * 64, 64, 64, 3))\n",
    "  # we loop on each column\n",
    "  for i in range(0,64):\n",
    "    # we loop on each row\n",
    "    for j in range(0,64):    \n",
    "      index = 64 * i + j\n",
    "      image_modified[index,:,:,:] = image\n",
    "      image_modified[index,i,j,k] = image[i,j,k] + delta\n",
    "  print(\"Evaluating differential images...\")\n",
    "  predicitions = np.zeros((64 * 64, 2))\n",
    "  print(f\"Computing predicitions for slice {k}...\")\n",
    "  predictions =  model.predict(image_modified)\n",
    "\n",
    "  derivatives = (predictions - initial_prediction) / delta\n",
    "\n",
    "  print(\"Calculating derivative image...\")\n",
    "  for i in range(0,64):\n",
    "    for j in range(0,64):\n",
    "      index = 64 * i + j\n",
    "      derivative[i,j,k]=derivatives[index][1]\n",
    "  # Free memory\n",
    "  image_modified = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjvDCjIWc5Mj"
   },
   "source": [
    "As the derivative has the shape of the original image, we can display is as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "LC62WH7k4AcL",
    "outputId": "5ec302f8-9a35-4af8-8e24-af5fe4a68a17"
   },
   "outputs": [],
   "source": [
    "# Values in derivative are small, so we need to amplify the signal\n",
    "plt.imshow(derivative*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui6JWxkcdG37"
   },
   "source": [
    "Then we use the derivative to compute a tricked image, and observe how the prediction evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "yyrTWX1Z4Xe4",
    "outputId": "3942fe42-199a-489d-f268-aea871e5bdd0"
   },
   "outputs": [],
   "source": [
    "print(train_iterator.class_indices)\n",
    "# Compute the tricked image by applying negatively the derivative\n",
    "tricked_image = np.minimum(1, np.maximum(image -  2 * derivative, 0))\n",
    "# Make the prediction on the tricked image\n",
    "print(model.predict(tricked_image.reshape((1,64,64,3))))\n",
    "# Display the tricked image\n",
    "plt.imshow(tricked_image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TBS_Image_classifcation_(student_version).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
